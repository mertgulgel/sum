{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwA0ujH94lSDeZ7t5SwCPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertgulgel/sum/blob/main/sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyMuPDF\n",
        "!pip install os\n",
        "!pip install PyPDF2\n",
        "!pip install re\n",
        "!pip install transformers\n",
        "!pip install pdfminer\n"
      ],
      "metadata": {
        "id": "swMP0Mdlr3sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install csv\n",
        "!pip install requests\n",
        "!pip install bs4"
      ],
      "metadata": {
        "id": "fs3GZigGsmz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "linkler = []\n",
        "\n",
        "with open('veriler.csv', mode='r', newline='', encoding='utf-8') as dosya:\n",
        "    okuyucu = csv.reader(dosya)\n",
        "    for satir in okuyucu:\n",
        "        link = satir[1]\n",
        "        linkler.append(link)\n",
        "\n",
        "for i in range(50):\n",
        "    url = linkler[i+1]\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(url)\n",
        "\n",
        "    download_link = []\n",
        "    pdf_link = soup.find('meta', {'name': 'citation_pdf_url'})\n",
        "    if pdf_link is not None:\n",
        "        pdf_url = pdf_link['content']\n",
        "\n",
        "        response = requests.get(pdf_url)\n",
        "        dergi_adi = pdf_url.split('/')[-1]\n",
        "        with open(f'pdfler/{dergi_adi}.pdf', 'wb') as dosya:\n",
        "            dosya.write(response.content)\n",
        "        print(f'{dergi_adi}.pdf indirildi')\n",
        "    else:\n",
        "        print(\"URL Not Found\")"
      ],
      "metadata": {
        "id": "7gbKa9IKsLFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber==0.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "3hNJQf54tujM",
        "outputId": "6800e39b-64d7-41d2-f66c-0892dd3e31ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber==0.11.0\n",
            "  Using cached pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber==0.11.0)\n",
            "  Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber==0.11.0) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber==0.11.0) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber==0.11.0) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber==0.11.0) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber==0.11.0) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber==0.11.0) (2.22)\n",
            "Installing collected packages: pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: pdfminer.six\n",
            "    Found existing installation: pdfminer.six 20200517\n",
            "    Uninstalling pdfminer.six-20200517:\n",
            "      Successfully uninstalled pdfminer.six-20200517\n",
            "  Attempting uninstall: pdfplumber\n",
            "    Found existing installation: pdfplumber 0.5.25\n",
            "    Uninstalling pdfplumber-0.5.25:\n",
            "      Successfully uninstalled pdfplumber-0.5.25\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pdfminer",
                  "pdfplumber"
                ]
              },
              "id": "5e16b3803ef0461a9e552df3742f67fb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "from pdfminer.pdfparser import PDFSyntaxError\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "# Daha küçük bir model seçmek için model ve tokenizer'ı değiştirin\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "pdf_folder_path = r'pdfler'\n",
        "text_folder_path = r'textler'\n",
        "\n",
        "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
        "text_files = [s for s in os.listdir(text_folder_path) if s.endswith('.txt')]\n",
        "\n",
        "summarization_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "uRrhv3xdgZA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b18dce2-9553-493d-9547-ed4df8d26cb6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "\n",
        "    except PDFSyntaxError as e:\n",
        "        print(f\"Syntax error reading {file_path}: {e}\")\n",
        "        # Attempt text extraction using PyMuPDF as an alternative method\n",
        "        try:\n",
        "            doc = fitz.open(file_path)\n",
        "            for page_number in range(len(doc)):\n",
        "                page = doc.load_page(page_number)\n",
        "                text += page.get_text(\"text\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text using PyMuPDF: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        # Handle other exceptions here\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "yUJ4KSfSr24S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
        "\n",
        "    try:\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        if len(pdf_text) != 0:\n",
        "            with open(f'textler/{pdf_file}.txt', 'w', encoding='utf-8') as dosya:\n",
        "                dosya.write(pdf_text)\n",
        "    except PDFSyntaxError as e:\n",
        "        print(f\"Syntax error reading {pdf_file}: {e}\")\n",
        "        print(f\"Skipping summarization for {pdf_file}.\")\n",
        "        continue\n",
        "\n",
        "    if not pdf_text:\n",
        "        print(f\"No text extracted from {pdf_file}. Skipping summarization.\")\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNMgDzZVtgKj",
        "outputId": "8f05a90f-c1ab-4e5c-f1b7-fb28414098a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No text extracted from 1172021.pdf. Skipping summarization.\n",
            "Syntax error reading pdfler/764679.pdf: No /Root object! - Is this really a PDF?\n",
            "Error extracting text using PyMuPDF: Failed to open file 'pdfler/764679.pdf'.\n",
            "No text extracted from 764679.pdf. Skipping summarization.\n",
            "No text extracted from 116579.pdf. Skipping summarization.\n",
            "No text extracted from 1097795.pdf. Skipping summarization.\n",
            "No text extracted from 38227.pdf. Skipping summarization.\n",
            "No text extracted from 945918.pdf. Skipping summarization.\n",
            "No text extracted from 436797.pdf. Skipping summarization.\n",
            "No text extracted from 321669.pdf. Skipping summarization.\n",
            "No text extracted from 1775985.pdf. Skipping summarization.\n",
            "No text extracted from 788938.pdf. Skipping summarization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_txt(txt_path):\n",
        "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "max_input_length = 512  # Max input length for the model\n",
        "\n",
        "for text_file in text_files:\n",
        "    text_path = os.path.join(text_folder_path, text_file)\n",
        "    print(text_path)\n",
        "    try:\n",
        "        text_text = extract_text_from_txt(text_path)\n",
        "        def preprocess_text(text):\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "            text = re.sub(r'\\d+|\\s+', '', text)\n",
        "            return text\n",
        "        processed_text = preprocess_text(text_text)\n",
        "        segments = [processed_text[i:i+max_input_length] for i in range(0, len(processed_text), max_input_length)]\n",
        "        summarized_segments = []\n",
        "        for segment in segments:\n",
        "            summarized_segment = summarization_pipeline(segment, max_length=49, min_length=30)[0]['summary_text']\n",
        "            summarized_segments.append(summarized_segment)\n",
        "        summarized_text = ' '.join(summarized_segments)\n",
        "        with open(f'summariez/{text_file}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write(summarized_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {text_file}: {e}\")\n",
        "        print(f\"Skipping summarization for {text_file}.\")\n",
        "        continue\n",
        "\n",
        "    if not text_text:\n",
        "        print(f\"No text extracted from {text_file}. Skipping summarization.\")\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzPSWhPxtise",
        "outputId": "a98569ba-77f3-438e-e852-4e6930154b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textler/2019455.pdf.txt\n"
          ]
        }
      ]
    }
  ]
}